# -*- coding: utf-8 -*-
"""SV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QLLihqf90Q6cJ5CsG3gew8Ax3rFP3b5s
"""

import torch
from torch import nn
import math
from torch.nn.parameter import Parameter
from torch.nn import functional as F
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import scipy.io as sio
from scipy.io import savemat
from sklearn.manifold import TSNE
import seaborn as sns
from torchvision.utils import save_image, make_grid
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from sklearn.manifold import TSNE
from sklearn.cluster import DBSCAN
import os

np.random.seed(4)
epochs =70
MEMDIM=200
batch=1

"""# **Data loading**"""

# load the data
d = sio.loadmat('HIAQ.mat') 
Xtrn = d['Xtrn']
Xtst = d['tst_f']
Xtst_real = d['tst_r']
print(Xtrn.shape)
print(Xtst.shape)
print(Xtst_real.shape)


mu, sigma = 0, 0.1 
# creating a noise with the same dimension as the dataset 
noise = np.random.normal(mu, sigma, [7000,5])  


X_noisy = Xtrn+3*noise
#con = np.concatenate((Xtrn, X_noisy[0:1000]))

df_n=pd.DataFrame(X_noisy, columns=['PM25', 'PM10', 'CO2', 'Temp','Humidity'])
df=pd.DataFrame(Xtrn, columns=['PM25', 'PM10', 'CO2', 'Temp','Humidity'])
df_faulty=pd.DataFrame(Xtst, columns=['PM25', 'PM10', 'CO2', 'Temp','Humidity'])
df_real= pd.DataFrame(Xtst_real, columns=['PM25', 'PM10', 'CO2', 'Temp','Humidity'])

from statsmodels.graphics.tsaplots import plot_acf

for b in df.columns:

  plot_acf(df[b],alpha=0.05)
  plt.title(f"Autocorrelation_{b}")
  plt.xlabel("Lag")
  plt.ylabel("Correlation value")
  plt.savefig(f'ACF_{b}.jpg', bbox_inches="tight", pad_inches=0.0)
  plt.clf()

"""print(X.shape)
print(XT_c.shape)
print(XT_r.shape)
print(len(df))

# **VAE**
"""

class Encoder(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(Encoder, self).__init__()

        self.FC_input = nn.Linear(input_dim, hidden_dim)
        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)
        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)
        self.FC_var   = nn.Linear (hidden_dim, latent_dim)
        
        self.LeakyReLU = nn.LeakyReLU(0.2)
        
        self.training = True
        
    def forward(self, x):
        h_       = self.LeakyReLU(self.FC_input(x))
        h_       = self.LeakyReLU(self.FC_input2(h_))
        mean     = self.FC_mean(h_)
        log_var  = self.FC_var(h_)                     # encoder produces mean and log of variance 
                                                       #             (i.e., parateters of simple tractable normal distribution "q"
        
        return mean, log_var
class Decoder(nn.Module):
    def __init__(self, latent_dim, hidden_dim, output_dim):
        super(Decoder, self).__init__()
        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)
        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)
        self.FC_output = nn.Linear(hidden_dim, output_dim)
        
        self.LeakyReLU = nn.LeakyReLU(0.2)
        
    def forward(self, x):
        h     = self.LeakyReLU(self.FC_hidden(x))
        h     = self.LeakyReLU(self.FC_hidden2(h))
        
        x_hat = self.FC_output(h)
        return x_hat

class Model(nn.Module):
    def __init__(self, Encoder, Decoder):
        super(Model, self).__init__()
        self.Encoder = Encoder
        self.Decoder = Decoder
        
    def reparameterization(self, mean, var):
        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        
        z = mean + var*epsilon                          # reparameterization trick
        return z
        
                
    def forward(self, x):
        mean, log_var = self.Encoder(x)
        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)
        x_hat            = self.Decoder(z)
        
        return {'output': x_hat, 'mean':mean, 'var':log_var, 'latent':z }   

encoder = Encoder(input_dim=5, hidden_dim=3, latent_dim=2)
decoder = Decoder(latent_dim=2, hidden_dim = 3, output_dim = 5)
DEVICE='cpu'
model_4 = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)      #VAE

class MemoryUnit(nn.Module):
    def __init__(self, mem_dim, fea_dim, shrink_thres=0.0025):
        super(MemoryUnit, self).__init__()
        self.mem_dim = mem_dim
        self.fea_dim = fea_dim
        self.weight = Parameter(torch.Tensor(self.mem_dim, self.fea_dim))  # M x C
        self.bias = None
        self.shrink_thres= shrink_thres
        # self.hard_sparse_shrink_opt = nn.Hardshrink(lambd=shrink_thres)

        self.reset_parameters()
    
    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input):
        att_weight = F.linear(input, self.weight)  # Fea x Mem^T, (TxC) x (CxM) = TxM   # memory data and encoder outputs generate att_w # transform
        att_weight = F.softmax(att_weight, dim=1)  # TxM
        # ReLU based shrinkage, hard shrinkage for positive value
        if(self.shrink_thres>0):
            att_weight = hard_shrink_relu(att_weight, lambd=self.shrink_thres)
            # att_weight = F.softshrink(att_weight, lambd=self.shrink_thres)
            # normalize???
            att_weight = F.normalize(att_weight, p=1, dim=1)
            # att_weight = F.softmax(att_weight, dim=1)
            # att_weight = self.hard_sparse_shrink_opt(att_weight)
        mem_trans = self.weight.permute(1, 0)  # Mem^T, MxC  #transpose
        output = F.linear(att_weight, mem_trans)  # AttWeight x Mem^T^T = AW x Mem, (TxM) x (MxC) = TxC
        return {'output': output, 'att': att_weight}  # output, att_weight

    def extra_repr(self):
        return 'mem_dim={}, fea_dim={}'.format(
            self.mem_dim, self.fea_dim is not None
        )


# NxCxHxW -> (NxHxW)xC -> addressing Mem, (NxHxW)xC -> NxCxHxW
class MemModule(nn.Module):
    def __init__(self, mem_dim, fea_dim, shrink_thres=0.0025, device='cuda'):
        super(MemModule, self).__init__()
        self.mem_dim = mem_dim
        self.fea_dim = fea_dim
        self.shrink_thres = shrink_thres
        self.memory = MemoryUnit(self.mem_dim, self.fea_dim, self.shrink_thres)
        self.melements=self.memory.weight
    def forward(self, input):
        s = input.data.shape
       # print(s)
        l = len(s)
        x = input.permute(0,1)
       # print(x)
        #print(x.shape)     
        y_and = self.memory(x)
       # print(y_and)
        #
        y = y_and['output']
       # print(y.shape)
        att = y_and['att']

        y = y.permute(0,1)
        att = att.view(s[0], self.mem_dim)
        att = att.permute(0, 1)
        #print("att")
        #print(att)
        return {'output': y, 'att': att}

# relu based hard shrinkage function, only works for positive values
def hard_shrink_relu(input, lambd=0, epsilon=1e-12):
    output = (F.relu(input-lambd) * input) / (torch.abs(input - lambd) + epsilon)
    return output

"""https://programmer.group/pytorch-learning-conv1d-conv2d-and-conv3d.html

https://towardsdatascience.com/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b

from sklearn import decomposition
pca = decomposition.PCA(n_components=2)
pca.fit(X_train_scaled)

# **MemAE**
"""

class MemAE(nn.Module):
    def __init__(self, mem_dim=MEMDIM, shrink_thres=0.0025):
        super(MemAE, self).__init__()
        print('MemAE')
    
        self.encoder = nn.Sequential(
            
            nn.Linear(5, 2),
            torch.nn.ReLU()
            
                     
            
        )
        self.mem_rep = MemModule(mem_dim=mem_dim, fea_dim=2, shrink_thres =shrink_thres)
        self.decoder = nn.Sequential(
            nn.Linear(2, 5)
           
           
           
                      
        )

    def forward(self, x):
        f = self.encoder(x)
        res_mem = self.mem_rep(f)
        f = res_mem['output']
        att = res_mem['att']
        output = self.decoder(f)
        return {'output': output, 'att':att, 'latent':f}

"""# **MVAE**"""

class MEncoder(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(MEncoder, self).__init__()

        self.FC_input = nn.Linear(input_dim, hidden_dim)
        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)
        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)
        self.FC_var   = nn.Linear (hidden_dim, latent_dim)
        
        self.LeakyReLU = nn.LeakyReLU(0.2)
        
        self.training = True
        
    def forward(self, x):
        h_       = self.LeakyReLU(self.FC_input(x))
        h_       = self.LeakyReLU(self.FC_input2(h_))
        mean     = self.FC_mean(h_)
        log_var  = self.FC_var(h_)                     # encoder produces mean and log of variance 
                                                       #             (i.e., parateters of simple tractable normal distribution "q"
        
        return mean, log_var
class MDecoder(nn.Module):
    def __init__(self, latent_dim, hidden_dim, output_dim):
        super(MDecoder, self).__init__()
        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)
        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)
        self.FC_output = nn.Linear(hidden_dim, output_dim)
        
        self.LeakyReLU = nn.LeakyReLU(0.2)
        
    def forward(self, x):
        h     = self.LeakyReLU(self.FC_hidden(x))
        h     = self.LeakyReLU(self.FC_hidden2(h))
        
        x_hat = self.FC_output(h)
        return x_hat

class MVAE(nn.Module):
    def __init__(self, Encoder, Decoder ,mem_dim=MEMDIM, shrink_thres=0.0025):
        super(MVAE, self).__init__()
        self.MEncoder = Encoder
        self.MDecoder = Decoder
        self.mem_rep = MemModule(mem_dim=mem_dim, fea_dim=2, shrink_thres =shrink_thres)
        
    def reparameterization(self, mean, var):
        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        
        z = mean + var*epsilon                          # reparameterization trick
        return z
        
                
    def forward(self, x):
        mean, log_var = self.MEncoder(x)
        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var

        res_mem = self.mem_rep(z)
        f = res_mem['output']
        att = res_mem['att']

        x_hat            = self.MDecoder(f)
        
        return {'output': x_hat, 'mean':mean, 'var':log_var, 'att':att, 'latent':f  }   

encoder = MEncoder(input_dim=5, hidden_dim=3, latent_dim=2)
decoder = MDecoder(latent_dim=2, hidden_dim = 3, output_dim = 5)
DEVICE='cpu'
model_5 = MVAE(Encoder=encoder, Decoder=decoder).to(DEVICE)      #MVAE

"""# **AE**"""

class AE(nn.Module):
    def __init__(self):
        super(AE, self).__init__()
        print('AE')
    
        self.encoder = nn.Sequential(
            nn.Linear(5, 2),
            torch.nn.ReLU()
                     
            
        )
       # self.mem_rep = MemModule(mem_dim=mem_dim, fea_dim=3, shrink_thres =shrink_thres)
        self.decoder = nn.Sequential(
            nn.Linear(2, 5)
           
           
        )

    def forward(self, x):
        f = self.encoder(x)
        output = self.decoder(f)
        return {'output': output, 'latent':f}

import torch
from torch import nn


def feature_map_permute(input):
    s = input.data.shape
    l = len(s)

    # permute feature channel to the last:
    # NxCxDxHxW --> NxDxHxW x C
    if l == 2:
        x = input # NxC
    elif l == 3:
        x = input.permute(0, 2, 1)
    elif l == 4:
        x = input.permute(0, 2, 3, 1)
    elif l == 5:
        x = input.permute(0, 2, 3, 4, 1)
    else:
        x = []
        print('wrong feature map size')
    x = x.contiguous()
    # NxDxHxW x C --> (NxDxHxW) x C
    x = x.view(-1, s[1])
    return x

class EntropyLoss(nn.Module):
    def __init__(self, eps = 1e-12):
        super(EntropyLoss, self).__init__()
        self.eps = eps

    def forward(self, x):
        b = x * torch.log(x + self.eps)
        b = -1.0 * b.sum(dim=1)
        b = b.mean()
        return b

class EntropyLossEncap(nn.Module):
    def __init__(self, eps = 1e-12):
        super(EntropyLossEncap, self).__init__()
        self.eps = eps
        self.entropy_loss = EntropyLoss(eps)

    def forward(self, input):
        score = feature_map_permute(input)
        ent_loss_val = self.entropy_loss(score)
        return ent_loss_val

if not os.path.exists('./loss'):
  os.mkdir('./loss')

model = MemAE()
print(model)
loss_function = torch.nn.MSELoss()
tr_entropy_loss_func = EntropyLossEncap()
optimizer = torch.optim.Adam(model.parameters(),
							lr = 0.0001)

if torch.cuda.is_available():
    device_name = torch.device("cuda")
else:
    device_name = torch.device('cpu')
print("Using {}.".format(device_name))

if not os.path.exists('./MemAE_Mem'):
  os.mkdir('./MemAE_Mem')

patience=10
#epochs =10
entropy_loss_weight=0.0002
wait=0
epoch_loss=[]
eog=False
for epoch in range(epochs):
  plt.scatter(model.mem_rep.melements.detach().numpy()[:,0],model.mem_rep.melements.detach().numpy()[:,1])
  #plt.show()
  plt.ylabel("Feature2")   
  plt.xlabel("Feature1")    
  plt.title("Memory_Elements_MemAE")
  plt.savefig(f'./MemAE_Mem/MemAE_{epoch}.jpg', bbox_inches="tight", pad_inches=0.0)
  plt.clf()
  if (eog==True):
    break
  losses=0 
  
  
  
  print(f"epoch: {epoch}") 
  b=int(len(df)/batch)
  for i in range(b) :
    
   # latent_mae=[]
   # outputs = []
    
    obs= torch.from_numpy(df.iloc[i*batch:(i+1)*batch].to_numpy())
    
    reconstructed=model(obs.float())
  #  print("obs")
  #  print(obs)
   # print("rec")
    #print(reconstructed['output'][0])
    att_w= reconstructed['att']
    #print(att_w)
    loss=  loss_function(reconstructed['output'], obs.float())
    recon_loss_val = loss.item()
    entropy_loss = tr_entropy_loss_func(att_w)
    entropy_loss_val = entropy_loss.item()
    loss = loss + entropy_loss_weight * entropy_loss
    loss_val = loss.item()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    losses= losses+loss_val
  epoch_loss.append(losses/b)   
  print(f"epoch_{epoch}_loss:  {losses/b}")
 
   
  if (epoch> patience):
  
    if (epoch_loss[epoch]>epoch_loss[epoch-1]):
      wait=wait+1
      if (wait>patience):
        print("End of training")
        eog=True
        torch.save(model.state_dict(),'./best_model_snap.pt' )
        
      else:
        wait=0

        

  if (epoch==epochs-1):
    torch.save(model.state_dict(),'./best_model_snap.pt' )	
   # sio.savemat("./latent_mae.mat", {"latent_mae": latent_mae})
   # sio.savemat("./output_mae.mat", {"outputs": outputs })   
  if (epoch%50==0):     
    torch.save(model.state_dict(),f'./model_snap_{epoch}.pt' )

plt.plot(epoch_loss)
plt.ylabel("Loss")   
plt.xlabel("Epoch")    
plt.title("Training loss for MemAE")
plt.savefig('./loss/loss_memae.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()

"""
  # outputs.append(reconstructed['output'].detach().numpy()[0] )
  # latent_mae.append(reconstructed['latent'].detach().numpy()[0])
 
  if (((epoch%50)== 0) or epoch==epochs-1):
    print(f"epoch: {epoch}")
    rec=np.array(outputs[:14000])
    tsne = TSNE(n_components=2)
    X_tsne = tsne.fit_transform(rec)
    df_tsne_2d.append(X_tsne)
   # df_tsne = pd.DataFrame(X_tsne, columns=["Dim1", "Dim2"])
    
   # df_tsne.head()
   # plt.figure(figsize=(8, 8))
   # sns.scatterplot(data=df_tsne, x="Dim1", y="Dim2")
   # plt.show()
 
 
    tsne = TSNE(n_components=3)
    X_tsne = tsne.fit_transform(rec)
    df_tsne_2.append(X_tsne)
    """
   # df_tsne_2.append(pd.DataFrame(X_tsne, columns=["Dim1", "Dim2","Dim3"]))

model_2 = AE()
print(model_2)
loss_function = torch.nn.MSELoss()
#tr_entropy_loss_func = EntropyLossEncap()
optimizer = torch.optim.Adam(model_2.parameters(),
							lr = 0.0001)

#model_2.to(device_name)

patience=10
#epochs =10000

wait=0
epoch_loss=[]
eog=False
for epoch in range(epochs):
  if (eog==True):
    break
  losses=0 
  
  
  
  print(f"epoch: {epoch}") 
  b=int(len(df)/batch)
  for i in range(b) :
    
   # latent_mae=[]
   # outputs = []
    obs= torch.from_numpy(df.iloc[i*batch:(i+1)*batch].to_numpy())
    
    reconstructed=model_2(obs.float())
  #  print("obs")
  #  print(obs)
   # print("rec")
    #print(reconstructed['output'][0])
    #att_w= reconstructed['att']
    #print(att_w)
    loss=  loss_function(reconstructed['output'], obs.float())
    recon_loss_val = loss.item()
    #entropy_loss = tr_entropy_loss_func(att_w)
   # entropy_loss_val = entropy_loss.item()
    #loss = loss + entropy_loss_weight * entropy_loss
    loss_val = loss.item()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    losses= losses+loss_val
  epoch_loss.append(losses/b)   
  print(f"epoch_{epoch}_loss:  {losses/b}")
 
   
  if (epoch> patience):
  
    if (epoch_loss[epoch]>epoch_loss[epoch-1]):
      wait=wait+1
      if (wait>patience):
        print("End of training")
        eog=True
        torch.save(model_2.state_dict(),'./best_model_2_snap.pt' )
        
      else:
        wait=0

        

  if (epoch==epochs-1):
    torch.save(model_2.state_dict(),'./best_model_2_snap.pt' )	
   # sio.savemat("./latent_mae.mat", {"latent_mae": latent_mae})
   # sio.savemat("./output_mae.mat", {"outputs": outputs })   
  if (epoch%50==0):     
    torch.save(model_2.state_dict(),f'./model_2_snap_{epoch}.pt' )

plt.plot(epoch_loss)
plt.plot(epoch_loss)
plt.ylabel("Loss")   
plt.xlabel("Epoch")    
plt.title("Training loss for AE")
plt.savefig('./loss/loss_ae.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()

"""
df_tsne_2=[]
df_tsne_2_2d=[]

#epochs =10000
entropy_loss_weight=0 #0.0002
for epoch in range(epochs):
  
  
  print(f"epoch: {epoch}") 
  for i in range(int(len(df)/batch)) :
    latent_ae=[]
    outputs = []
    obs= torch.from_numpy(df.iloc[i*batch:(i+1)*batch].to_numpy())
   
    reconstructed=model_2(obs.float())
  #  print("obs")
  #  print(obs)
   # print("rec")
    #print(reconstructed['output'][0])
    #att_w= reconstructed['att']
    #print(att_w)
    loss=  loss_function(reconstructed['output'], obs.float())
    recon_loss_val = loss.item()
   # entropy_loss = tr_entropy_loss_func(att_w)
    #entropy_loss_val = entropy_loss.item()
    #loss = loss + entropy_loss_weight * entropy_loss
    loss_val = loss.item()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    losses.append(loss_val)

    outputs.append(reconstructed['output'].detach().numpy()[0] )
    latent_ae.append(reconstructed['latent'].detach().numpy()[0])

    
  
  if (((epoch%50)== 0) or epoch==epochs-1):
    print(f"epoch: {epoch}")
    rec=np.array(outputs[:14000])
    tsne = TSNE(n_components=2)
    X_tsne = tsne.fit_transform(rec)
    df_tsne_2_2d.append(X_tsne)
   # df_tsne = pd.DataFrame(X_tsne, columns=["Dim1", "Dim2"])
    
   # df_tsne.head()
   # plt.figure(figsize=(8, 8))
   # sns.scatterplot(data=df_tsne, x="Dim1", y="Dim2")
   # plt.show()
  
   
   # df_tsne_2.append(pd.DataFrame(X_tsne, columns=["Dim1", "Dim2","Dim3"]))
    


  if (epoch==epochs-1):
    torch.save(model_2.state_dict(),'./model_2_snap.pt' )	
    sio.savemat("./latent_ae.mat", {"latent_ae": latent_ae})
    sio.savemat("./output_ae.mat", {"outputs": outputs })

  if (epoch%50==0):     
    torch.save(model_2.state_dict(),f'./model_2_snap_{epoch}.pt' )	
  """

model_3 = AE() #denoising autoencoder
print(model_3)
loss_function = torch.nn.MSELoss()
#tr_entropy_loss_func = EntropyLossEncap()
optimizer = torch.optim.Adam(model_3.parameters(),
							lr = 0.0001)

patience=10
#epochs =10000

wait=0
epoch_loss=[]
eog=False
for epoch in range(epochs):
  if (eog==True):
    break
  losses=0 
  
  
  
  print(f"epoch: {epoch}") 
  b=int(len(df)/batch)
  for i in range(b) :
    
   # latent_mae=[]
   # outputs = []
    obs= torch.from_numpy(df.iloc[i*batch:(i+1)*batch].to_numpy())
    n_obs= torch.from_numpy(df_n.iloc[i*batch:(i+1)*batch].to_numpy())
    reconstructed=model_3(n_obs.float())
  #  print("obs")
  #  print(obs)
   # print("rec")
    #print(reconstructed['output'][0])
    #att_w= reconstructed['att']
    #print(att_w)
    loss=  loss_function(reconstructed['output'], obs.float())
    recon_loss_val = loss.item()
    #entropy_loss = tr_entropy_loss_func(att_w)
   # entropy_loss_val = entropy_loss.item()
    #loss = loss + entropy_loss_weight * entropy_loss
    loss_val = loss.item()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    losses= losses+loss_val
  epoch_loss.append(losses/b)   
  print(f"epoch_{epoch}_loss:  {losses/b}")
 
   
  if (epoch> patience):
  
    if (epoch_loss[epoch]>epoch_loss[epoch-1]):
      wait=wait+1
      if (wait>patience):
        print("End of training")
        eog=True
        torch.save(model_3.state_dict(),'./best_model_3_snap.pt' )
        
      else:
        wait=0

        

  if (epoch==epochs-1):
    torch.save(model_3.state_dict(),'./best_model_3_snap.pt' )	
   # sio.savemat("./latent_mae.mat", {"latent_mae": latent_mae})
   # sio.savemat("./output_mae.mat", {"outputs": outputs })   
  if (epoch%50==0):     
    torch.save(model_3.state_dict(),f'./model_3_snap_{epoch}.pt' )

plt.plot(epoch_loss)

plt.plot(epoch_loss)
plt.ylabel("Loss")   
plt.xlabel("Epoch")    
plt.title("Training loss for DAE")
plt.savefig('./loss/loss_dae.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()

"""
df_tsne_3=[]
df_tsne_3_2d=[]

#epochs =10000
entropy_loss_weight=0 #0.0002
for epoch in range(epochs):
  
  
  print(f"epoch: {epoch}") 
  for i in range(int(len(df)/batch)) :
    latent_dae=[]
    outputs = []
    obs= torch.from_numpy(df.iloc[i*batch:(i+1)*batch].to_numpy())
    n_obs= torch.from_numpy(df_n.iloc[i*batch:(i+1)*batch].to_numpy())
    
    reconstructed=model_3(n_obs.float())
    #print(n_obs.float().shape)
    #print(reconstructed['output'].shape)
  #  print("obs")
  #  print(obs)
   # print("rec")
    #print(reconstructed['output'][0])
    #att_w= reconstructed['att']
    #print(att_w)
    loss=  loss_function(reconstructed['output'], obs.float())
    recon_loss_val = loss.item()
   # entropy_loss = tr_entropy_loss_func(att_w)
    #entropy_loss_val = entropy_loss.item()
    #loss = loss + entropy_loss_weight * entropy_loss
    loss_val = loss.item()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    losses.append(loss_val)

    outputs.append(reconstructed['output'].detach().numpy()[0] )
    latent_dae.append(reconstructed['latent'].detach().numpy()[0])
  
  if (((epoch%50)== 0) or epoch==epochs-1):
    print(f"epoch: {epoch}")
    rec=np.array(outputs[:14000])
    tsne = TSNE(n_components=2)
    X_tsne = tsne.fit_transform(rec)
    df_tsne_2_2d.append(X_tsne)
   # df_tsne = pd.DataFrame(X_tsne, columns=["Dim1", "Dim2"])
    
   # df_tsne.head()
   # plt.figure(figsize=(8, 8))
   # sns.scatterplot(data=df_tsne, x="Dim1", y="Dim2")
   # plt.show()
   
    tsne = TSNE(n_components=3)
    X_tsne = tsne.fit_transform(rec)
    df_tsne_2.append(X_tsne)
   # df_tsne_2.append(pd.DataFrame(X_tsne, columns=["Dim1", "Dim2","Dim3"]))
    
    

  if (epoch==epochs-1):
    torch.save(model_3.state_dict(),'./model_3_snap.pt' )
    sio.savemat("./latent_dae.mat", {"latent_dae": latent_dae})
    sio.savemat("./output_dae.mat", {"outputs": outputs }) 
   		
  if (epoch%50==0):     
    torch.save(model_3.state_dict(),f'./model_3_snap_{epoch}.pt' )
   """

#model_4 = Model() #VAE
print(model_4)
loss_function = torch.nn.MSELoss()
#tr_entropy_loss_func = EntropyLossEncap()
optimizer = torch.optim.Adam(model_4.parameters(),
							lr = 0.0001)

patience=10
#epochs =20

wait=0
epoch_loss=[]
eog=False
for epoch in range(epochs):
  if (eog==True):
    break
  losses=0 
  
  
  
  print(f"epoch: {epoch}") 
  b=int(len(df)/batch)
  for i in range(b) :
    
   # latent_mae=[]
   # outputs = []
    obs= torch.from_numpy(df.iloc[i*batch:(i+1)*batch].to_numpy())
   # n_obs= torch.from_numpy(df_n.iloc[i*batch:(i+1)*batch].to_numpy())
    reconstructed=model_4(obs.float())
  #  print("obs")
  #  print(obs)
   # print("rec")
    #print(reconstructed['output'][0])
    #att_w= reconstructed['att']
    #print(att_w)
    loss=  loss_function(reconstructed['output'], obs.float())
    recon_loss_val = loss.item()
    #entropy_loss = tr_entropy_loss_func(att_w)
   # entropy_loss_val = entropy_loss.item()
    #loss = loss + entropy_loss_weight * entropy_loss
    loss_val = loss.item()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    losses= losses+loss_val
  epoch_loss.append(losses/b)   
  print(f"epoch_{epoch}_loss:  {losses/b}")
 
   
  if (epoch> patience):
  
    if (epoch_loss[epoch]>epoch_loss[epoch-1]):
      wait=wait+1
      if (wait>patience):
        print("End of training")
        eog=True
        torch.save(model_4.state_dict(),'./best_model_4_snap.pt' )
        
      else:
        wait=0

        

  if (epoch==epochs-1):
    torch.save(model_4.state_dict(),'./best_model_4_snap.pt' )	
   # sio.savemat("./latent_mae.mat", {"latent_mae": latent_mae})
   # sio.savemat("./output_mae.mat", {"outputs": outputs })   
  if (epoch%50==0):     
    torch.save(model_4.state_dict(),f'./model_4_snap_{epoch}.pt' )

plt.plot(epoch_loss)

plt.plot(epoch_loss)
plt.ylabel("Loss")   
plt.xlabel("Epoch")    
plt.title("Training loss for VAE")
plt.savefig('./loss/loss_vae.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()

"""
df_tsne_4=[]
df_tsne_4_2d=[]
#epochs =200
entropy_loss_weight=0 #0.0002
for epoch in range(epochs):
  outputs = []
  latent_vae=[]
  print(f"epoch: {epoch}") 
  for i in range(int(len(df)/batch)) :
    obs= torch.from_numpy(df.iloc[i*batch:(i+1)*batch].to_numpy())
   # n_obs= torch.from_numpy(df_n.iloc[i*batch:(i+1)*batch].to_numpy())
    
    reconstructed=model_4(obs.float())
  #  print(n_obs.float().shape)
   # print(reconstructed['output'].shape)
  #  print("obs")
  #  print(obs)
   # print("rec")
    #print(reconstructed['output'][0])
    #att_w= reconstructed['att']
    #print(att_w)
    loss=  loss_function(reconstructed['output'], obs.float())
    recon_loss_val = loss.item()
   # entropy_loss = tr_entropy_loss_func(att_w)
    #entropy_loss_val = entropy_loss.item()
    #loss = loss + entropy_loss_weight * entropy_loss
    loss_val = loss.item()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    losses.append(loss_val)

    outputs.append(reconstructed['output'].detach().numpy()[0] )
   # latent_dae.append(reconstructed['latent'].detach().numpy()[0])
  
  if (((epoch%50)== 0) or epoch==epochs-1):
    print(f"epoch: {epoch}")
    rec=np.array(outputs[:14000])
    tsne = TSNE(n_components=2)
    X_tsne = tsne.fit_transform(rec)
    df_tsne_2_2d.append(X_tsne)
   # df_tsne = pd.DataFrame(X_tsne, columns=["Dim1", "Dim2"])
    
   # df_tsne.head()
   # plt.figure(figsize=(8, 8))
   # sns.scatterplot(data=df_tsne, x="Dim1", y="Dim2")
   # plt.show()
  
  
    tsne = TSNE(n_components=3)
    X_tsne = tsne.fit_transform(rec)
    df_tsne_2.append(X_tsne)
   # df_tsne_2.append(pd.DataFrame(X_tsne, columns=["Dim1", "Dim2","Dim3"]))
    
    

  if (epoch==epochs-1):
    torch.save(model_4.state_dict(),'./model_4_snap.pt' )	
  if (epoch%50==0):     
    torch.save(model_4.state_dict(),f'./model_4_snap_{epoch}.pt' )	

    """

#model_5 = Model() #MVAE
print(model_5)
loss_function = torch.nn.MSELoss()
#tr_entropy_loss_func = EntropyLossEncap()
optimizer = torch.optim.Adam(model_5.parameters(),
							lr = 0.0001)

if not os.path.exists('./MVAE_Mem'):
  os.mkdir('./MVAE_Mem')

patience=10
#epochs =10000
entropy_loss_weight=0.0002
wait=0
epoch_loss=[]
eog=False
for epoch in range(epochs):
  plt.scatter(model.mem_rep.melements.detach().numpy()[:,0],model.mem_rep.melements.detach().numpy()[:,1])
  #plt.show()
  plt.ylabel("Feature2")   
  plt.xlabel("Feature1")    
  plt.title("Memory_Elements_MVAE")
  plt.savefig(f'./MVAE_Mem/MVAE_{epoch}.jpg', bbox_inches="tight", pad_inches=0.0)
  plt.clf()
  if (eog==True):
    break
  losses=0 
  
  
  
  print(f"epoch: {epoch}") 
  b=int(len(df)/batch)
  for i in range(b) :
    
   # latent_mae=[]
   # outputs = []
    
    obs= torch.from_numpy(df.iloc[i*batch:(i+1)*batch].to_numpy())
   # n_obs= torch.from_numpy(df_n.iloc[i*batch:(i+1)*batch].to_numpy())
    reconstructed=model_5(obs.float())
  #  print("obs")
  #  print(obs)
   # print("rec")
    #print(reconstructed['output'][0])
    att_w= reconstructed['att']
    #print(att_w)
    loss=  loss_function(reconstructed['output'], obs.float())
    recon_loss_val = loss.item()
    entropy_loss = tr_entropy_loss_func(att_w)
    entropy_loss_val = entropy_loss.item()
    loss = loss + entropy_loss_weight * entropy_loss
    loss_val = loss.item()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    losses= losses+loss_val
  epoch_loss.append(losses/b)   
  print(f"epoch_{epoch}_loss:  {losses/b}")
 
   
  if (epoch> patience):
  
    if (epoch_loss[epoch]>epoch_loss[epoch-1]):
      wait=wait+1
      if (wait>patience):
        print("End of training")
        eog=True
        torch.save(model_5.state_dict(),'./best_model_5_snap.pt' )
        
      else:
        wait=0

        

  if (epoch==epochs-1):
    torch.save(model_5.state_dict(),'./best_model_5_snap.pt' )	
   # sio.savemat("./latent_mae.mat", {"latent_mae": latent_mae})
   # sio.savemat("./output_mae.mat", {"outputs": outputs })   
  if (epoch%50==0):     
    torch.save(model_5.state_dict(),f'./model_5_snap_{epoch}.pt' )

plt.plot(epoch_loss)

plt.plot(epoch_loss)
plt.ylabel("Loss")   
plt.xlabel("Epoch")    
plt.title("Training loss for MVAE")
plt.savefig('./loss/loss_mvae.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()

"""df_tsne_5=[]
df_tsne_5_2d=[]
#epochs =1000
entropy_loss_weight=0.0002
for epoch in range(epochs):
  outputs = []
  latent_mvae=[]
  print(f"epoch: {epoch}") 
  for i in range(int(len(df)/batch)) :
    obs= torch.from_numpy(df.iloc[i*batch:(i+1)*batch].to_numpy())
   # n_obs= torch.from_numpy(df_n.iloc[i*batch:(i+1)*batch].to_numpy())
    
    reconstructed=model_5(obs.float())
  #  print(n_obs.float().shape)
   # print(reconstructed['output'].shape)
  #  print("obs")
  #  print(obs)
   # print("rec")
    #print(reconstructed['output'][0])
    att_w= reconstructed['att']
    #print(att_w)
    loss=  loss_function(reconstructed['output'], obs.float())
    recon_loss_val = loss.item()
    entropy_loss = tr_entropy_loss_func(att_w)
    entropy_loss_val = entropy_loss.item()
    loss = loss + entropy_loss_weight * entropy_loss
    loss_val = loss.item()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    losses.append(loss_val)

    outputs.append(reconstructed['output'].detach().numpy()[0] )
   # latent_dae.append(reconstructed['latent'].detach().numpy()[0])
    
  if (((epoch%50)== 0) or epoch==epochs-1):
    print(f"epoch: {epoch}")
    rec=np.array(outputs[:14000])
    tsne = TSNE(n_components=2)
    X_tsne = tsne.fit_transform(rec)
    df_tsne_2_2d.append(X_tsne)
   # df_tsne = pd.DataFrame(X_tsne, columns=["Dim1", "Dim2"])
    
   # df_tsne.head()
   # plt.figure(figsize=(8, 8))
   # sns.scatterplot(data=df_tsne, x="Dim1", y="Dim2")
   # plt.show()
   
    tsne = TSNE(n_components=3)
    X_tsne = tsne.fit_transform(rec)
    df_tsne_2.append(X_tsne)
   # df_tsne_2.append(pd.DataFrame(X_tsne, columns=["Dim1", "Dim2","Dim3"]))
    
    

  if (epoch==epochs-1):
    torch.save(model_5.state_dict(),'./model_5_snap.pt' )	
  if (epoch%50==0):     
    torch.save(model_5.state_dict(),f'./model_5_snap_{epoch}.pt' )

https://datascience.stackexchange.com/questions/34146/what-does-the-long-curve-shape-t-sne-mean

tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(Xtrn)

df_tsne_t = pd.DataFrame(X_tsne, columns=["Dim1", "Dim2"])
    


for c in range(len(df_tsne_2_2d)):
  df_memae= pd.DataFrame()
  df_ae= pd.DataFrame()
  
  df_memae=pd.DataFrame(df_tsne_2d[c], columns=["Dim1", "Dim2"])
  df_ae=pd.DataFrame(df_tsne_2_2d[c], columns=["Dim1", "Dim2"])
 
  plt.figure(figsize=(8,8))
  sns.scatterplot(data=df_memae, x="Dim1", y="Dim2",   label='MemAE')
  sns.scatterplot(data=df_ae, x="Dim1", y="Dim2",  label='AE')
  #sns.scatterplot(data=df_tsne_t, x="Dim1", y="Dim2", label='Original')


  plt.legend()
  plt.savefig(f'./epoch_2d{c}.jpg', bbox_inches="tight", pad_inches=0.0) 
  #plt.clf()
  plt.show()
"""

model_para = torch.load('./best_model_snap.pt') #memae
model.load_state_dict(model_para)

model_para = torch.load('./best_model_2_snap.pt') #ae
model_2.load_state_dict(model_para)

model_para = torch.load('./best_model_3_snap.pt') #dae
model_3.load_state_dict(model_para)

model_para = torch.load('./best_model_4_snap.pt') #dae
model_4.load_state_dict(model_para)

model_para = torch.load('./best_model_5_snap.pt') #dae
model_5.load_state_dict(model_para)

"""def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):
    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)
    core_mask[dbscan.core_sample_indices_] = True
    anomalies_mask = dbscan.labels_ == -1
    non_core_mask = ~(core_mask | anomalies_mask)

    cores = dbscan.components_
    anomalies = X[anomalies_mask]
    non_cores = X[non_core_mask]
    plt.figure(figsize = (5,4), dpi = 450)
    plt.scatter(cores[:, 0], cores[:, 1],
                c=dbscan.labels_[core_mask], marker='o', s=size, cmap="Paired")
    # plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=15, c=dbscan.labels_[core_mask])
    # plt.scatter(anomalies[:, 0], anomalies[:, 1], c="r", marker="x", s=100)
    # plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=".")
    if show_xlabels:
        plt.xlabel("t-SNE parameter 1", fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel("t-SNE parameter 2", fontsize=14)
    else:
        plt.tick_params(labelleft=False)

latent_mae=np.array(latent_mae)
d = sio.loadmat('latent_mae.mat') 
print(d['latent_mae'])
"""

bars=df.columns

rec_train=[]
rec_train_2=[]
rec_train_3=[]
rec_train_4=[]
rec_train_5=[]
lat_mae=[]
lat_ae=[]
lat_dae=[]
lat_vae=[]
lat_mvae=[]

c=len(df)

for i in range(len(df)) :
    obs= torch.from_numpy(df.iloc[i+0:i+1].to_numpy())
    reconstructed=model(obs.float())
    reconstructed_2=model_2(obs.float())
    reconstructed_3=model_3(obs.float())
    reconstructed_4=model_4(obs.float())
    reconstructed_5=model_5(obs.float())
    
    rec_train.append(reconstructed['output'].detach().numpy()[0] )
    lat_mae.append(reconstructed['latent'].detach().numpy()[0])
    rec_train_2.append(reconstructed_2['output'].detach().numpy()[0] )
    lat_ae.append(reconstructed_2['latent'].detach().numpy()[0])
    rec_train_3.append(reconstructed_3['output'].detach().numpy()[0] )
    lat_dae.append(reconstructed_3['latent'].detach().numpy()[0])
    rec_train_4.append(reconstructed_4['output'].detach().numpy() [0]  )
    lat_vae.append(reconstructed_4['latent'].detach().numpy()[0])
    rec_train_5.append(reconstructed_5['output'].detach().numpy()[0] )
    lat_mvae.append(reconstructed_5['latent'].detach().numpy()[0])
  
df_rec_memae=pd.DataFrame(rec_train, columns=df.columns)
df_rec_ae=pd.DataFrame(rec_train_2, columns=df.columns)
df_rec_dae=pd.DataFrame(rec_train_3, columns=df.columns)
df_rec_vae=pd.DataFrame(rec_train_4, columns=df.columns)
df_rec_mvae=pd.DataFrame(rec_train_5, columns=df.columns)

models=['memae','ae','dae','vae','mvae']
 #print(reconstructed['output'].detach().numpy()[0])
if not os.path.exists('./train'):
  os.mkdir('./train') 
for b in df.columns:
  for m in models:

    
    if m=='memae':
        plt.plot(df_rec_memae[b].iloc[0:700],linestyle = 'dotted', color='red',label='Reconstructed_MemAE',marker='.')

        plt.plot(df[b].iloc[0:700], label='Actual',color='blue', marker='.')
        plt.legend()
        plt.xlabel(f"{b}_train_{m}")
        plt.title("train")
        plt.savefig(f'./train/{b}_train_{m}.jpg', bbox_inches="tight", pad_inches=0.0)
        plt.clf()  
    elif m=='ae':
        plt.plot(df_rec_ae[b].iloc[0:700],linestyle = 'dotted', color='red',label='Reconstructed_AE',marker='.') 

        plt.plot(df[b].iloc[0:700], label='Actual',color='blue', marker='.')
        plt.legend()
        plt.xlabel(f"{b}_train_{m}")
        plt.title("train")
        plt.savefig(f'./train/{b}_train_{m}.jpg', bbox_inches="tight", pad_inches=0.0)
        plt.clf()  
    elif m=='dae':
        plt.plot(df_rec_dae[b].iloc[0:700],linestyle = 'dotted', color='red',label='Reconstructed_DAE',marker='.')

        plt.plot(df[b].iloc[0:700], label='Actual',color='blue', marker='.')
        plt.legend()
        plt.xlabel(f"{b}_train_{m}")
        plt.title("train")
        plt.savefig(f'./train/{b}_train_{m}.jpg', bbox_inches="tight", pad_inches=0.0)
        plt.clf()   
    elif m=='vae':
        plt.plot(df_rec_vae[b].iloc[0:700],linestyle = 'dotted', color='red',label='Reconstructed_VAE',marker='.')   

        plt.plot(df[b].iloc[0:700], label='Actual',color='blue', marker='.')
        plt.legend()
        plt.xlabel(f"{b}_train_{m}")
        plt.title("train")
        plt.savefig(f'./train/{b}_train_{m}.jpg', bbox_inches="tight", pad_inches=0.0)
        plt.clf()  
    elif m=='mvae':
        plt.plot(df_rec_mvae[b].iloc[0:700],linestyle = 'dotted', color='red',label='Reconstructed_MVAE',marker='.')

        plt.plot(df[b].iloc[0:700], label='Actual',color='blue', marker='.')
        plt.legend()
        plt.xlabel(f"{b}_train_{m}")
        plt.title("train")
        plt.savefig(f'./train/{b}_train_{m}.jpg', bbox_inches="tight", pad_inches=0.0)
        plt.clf()    


   # plt.plot(df_rec[b].iloc[0:700],linestyle = 'dotted', color='red',label='Reconstructed_MemAE',marker='.')
   # plt.plot(df_rec_2[b].iloc[0:700],linestyle = 'dotted', color='pink',label='Reconstructed_AE',marker='.')
   # plt.plot(df_rec_3[b].iloc[0:700],linestyle = 'dotted', color='green',label='Reconstructed_DAE',marker='.')
   # plt.plot(df_rec_4[b].iloc[0:700],linestyle = 'dotted', color='orange',label='Reconstructed_VAE',marker='.')
   # plt.plot(df_rec_5[b].iloc[0:700],linestyle = 'dotted', color='orange',label='Reconstructed_MVAE',marker='.')

   # plt.legend()
    #plt.xlabel(b+"_train")
    #plt.show()
   # plt.title("train")
   # plt.savefig(f'./train_models_{b}.jpg', bbox_inches="tight", pad_inches=0.0) 
    #plt.show()
    #plt.clf()    

rec=np.array(rec_train[:7000])
sio.savemat("./reconst_train_memae.mat", {"z_train":  rec})
rec=np.array(rec_train_2[:7000])
sio.savemat("./reconst_train_ae.mat", {"z_train":  rec})
rec=np.array(rec_train_3[:7000])
sio.savemat("./reconst_train_dae.mat", {"z_train":  rec})
rec=np.array(rec_train_4[:7000])
sio.savemat("./reconst_train_vae.mat", {"z_train":  rec})
rec=np.array(rec_train_5[:7000])
sio.savemat("./reconst_train_mvae.mat", {"z_train":  rec})


rec=np.array(lat_mae[:7000])
sio.savemat("./reconst_latent_memae.mat", {"latent":  rec})
rec=np.array(lat_ae[:7000])
sio.savemat("./reconst_latent_ae.mat", {"latent":  rec})
rec=np.array(lat_dae[:7000])
sio.savemat("./reconst_latent_dae.mat", {"latent":  rec})
rec=np.array(lat_vae[:7000])
sio.savemat("./reconst_latent_vae.mat", {"latent":  rec})
rec=np.array(lat_mvae[:7000])
sio.savemat("./reconst_latent_mvae.mat", {"latent":  rec})
#data22 = sio.loadmat('reconst_train.mat')
#print(rec[:,2])

rec_test=[]
rec_test_2=[]
rec_test_3=[]
rec_test_4=[]
rec_test_5=[]

lat_mae=[]
lat_ae=[]
lat_dae=[]
lat_vae=[]
lat_mvae=[]


for i in range(len(df_faulty)) :
      obs= torch.from_numpy(df_faulty.iloc[i+0:i+1].to_numpy())
      reconstructed=model(obs.float())
      reconstructed_2=model_2(obs.float())
      reconstructed_3=model_3(obs.float())
      reconstructed_4=model_4(obs.float())
      reconstructed_5=model_5(obs.float())

      rec_test.append(reconstructed['output'].detach().numpy()[0] )
      lat_mae.append(reconstructed['latent'].detach().numpy()[0])
      rec_test_2.append(reconstructed_2['output'].detach().numpy()[0] )
      lat_ae.append(reconstructed_2['latent'].detach().numpy()[0])
      rec_test_3.append(reconstructed_3['output'].detach().numpy()[0] )
      lat_dae.append(reconstructed_3['latent'].detach().numpy()[0])
      rec_test_4.append(reconstructed_4['output'].detach().numpy()[0] )
      lat_vae.append(reconstructed_4['latent'].detach().numpy()[0])
      rec_test_5.append(reconstructed_5['output'].detach().numpy()[0] )
      lat_mvae.append(reconstructed_5['latent'].detach().numpy()[0])
    # print(reconstructed['att'])
#print(rec_test_4)    
df_rec=pd.DataFrame(rec_test, columns=df.columns)
df_rec_2=pd.DataFrame(rec_test_2, columns=df.columns)
df_rec_3=pd.DataFrame(rec_test_3, columns=df.columns)
df_rec_4=pd.DataFrame(rec_test_4, columns=df.columns)
df_rec_5=pd.DataFrame(rec_test_5, columns=df.columns)
  #print(reconstructed['output'].detach().numpy()[0])


rec=np.array(rec_test)
sio.savemat("./reconst_test_memae.mat", {"z_test": rec})  

rec=np.array(rec_test_2)
sio.savemat("./reconst_test_ae.mat", {"z_test": rec}) 

rec=np.array(rec_test_3)
sio.savemat("./reconst_test_dae.mat", {"z_test": rec}) 

rec=np.array(rec_test_4)
sio.savemat("./reconst_test_vae.mat", {"z_test": rec}) 

rec=np.array(rec_test_5)
sio.savemat("./reconst_test_mvae.mat", {"z_test": rec}) 




rec=np.array(lat_mvae)
sio.savemat("./reconst_test_latent_memae.mat", {"latent_t":  rec})
rec=np.array(lat_ae)
sio.savemat("./reconst_test_latent_ae.mat", {"latent_t":  rec})
rec=np.array(lat_dae)
sio.savemat("./reconst_test_latent_dae.mat", {"latent_t":  rec})
rec=np.array(lat_vae)
sio.savemat("./reconst_test_latent_vae.mat", {"latent_t":  rec})
rec=np.array(lat_mvae)
sio.savemat("./reconst_test_latent_mvae.mat", {"latent_t":  rec})

"""just run for each model everytime to get better fig"""

if not os.path.exists('./test'):
  os.mkdir('./test') 

for b in df.columns:
      plt.figure(figsize=(10, 6))
      plt.plot(df_real[b].iloc[0:700], label='Actual',color='blue', alpha=0.5,marker='.')
      plt.plot(df_faulty[b].iloc[0:700], label='Faulty', color='black',alpha=0.6,marker='.')  

      ##plt.plot(df_rec[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_MemAE',alpha=1,marker="8")   # just run for each model everytime to get better fig
      #plt.plot(df_rec_2[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_AE',alpha=0.8,marker="8")
      plt.plot(df_rec_5[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_MVAE',alpha=0.9,marker="8")
    
      plt.legend()
      plt.xlabel(b+"_test")
      
      plt.title("test")
      plt.savefig(f'./test/test_mvae_{b}.jpg', bbox_inches="tight", pad_inches=0.0)
     #plt.show() 
      plt.clf()

for b in df.columns:
      plt.figure(figsize=(10, 6))
      plt.plot(df_real[b].iloc[0:700], label='Actual',color='blue', alpha=0.5,marker='.')
      plt.plot(df_faulty[b].iloc[0:700], label='Faulty', color='black',alpha=0.6,marker='.')  

      ##plt.plot(df_rec[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_MemAE',alpha=1,marker="8")   # just run for each model everytime to get better fig
      #plt.plot(df_rec_2[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_AE',alpha=0.8,marker="8")
      plt.plot(df_rec_4[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_VAE',alpha=0.9,marker="8")
    
      plt.legend()
      plt.xlabel(b+"_test")
      
      plt.title("test")
      plt.savefig(f'./test/test_vae_{b}.jpg', bbox_inches="tight", pad_inches=0.0)
     # plt.show() 
      plt.clf()

for b in df.columns:
      plt.figure(figsize=(10, 6))
      plt.plot(df_real[b].iloc[0:700], label='Actual',color='blue', alpha=0.5,marker='.')
      plt.plot(df_faulty[b].iloc[0:700], label='Faulty', color='black',alpha=0.6,marker='.')  

      plt.plot(df_rec[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_MemAE',alpha=1,marker="8")   # just run for each model everytime to get better fig
      #plt.plot(df_rec_2[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_AE',alpha=0.8,marker="8")
      #plt.plot(df_rec_4[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_VAE',alpha=0.9,marker="8")
    
      plt.legend()
      plt.xlabel(b+"_test")
      
      plt.title("test")
      plt.savefig(f'./test/test_memae_{b}.jpg', bbox_inches="tight", pad_inches=0.0)
      #plt.show() 
      plt.clf()

for b in df.columns:
      plt.figure(figsize=(10, 6))
      plt.plot(df_real[b].iloc[0:700], label='Actual',color='blue', alpha=0.5,marker='.')
      plt.plot(df_faulty[b].iloc[0:700], label='Faulty', color='black',alpha=0.6,marker='.')  

      #plt.plot(df_rec[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_MemAE',alpha=1,marker="8")   # just run for each model everytime to get better fig
      plt.plot(df_rec_2[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_AE',alpha=0.8,marker="8")
      #plt.plot(df_rec_3[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_DAE',alpha=0.9,marker="8")
    
      plt.legend()
      plt.xlabel(b+"_test")
      
      plt.title("test")
      plt.savefig(f'./test/test_ae_{b}.jpg', bbox_inches="tight", pad_inches=0.0)
     # plt.show() 
      plt.clf()

for b in df.columns:
      plt.figure(figsize=(10, 6))
      plt.plot(df_real[b].iloc[0:700], label='Actual',color='blue', alpha=0.5,marker='.')
      plt.plot(df_faulty[b].iloc[0:700], label='Faulty', color='black',alpha=0.6,marker='.')  

      #plt.plot(df_rec[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_MemAE',alpha=1,marker="8")   # just run for each model everytime to get better fig
      #plt.plot(df_rec_2[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_AE',alpha=0.8,marker="8")
      plt.plot(df_rec_3[b].iloc[0:700],linestyle = 'dotted', color='darkred',label='Reconstructed_DAE',alpha=0.9,marker="8")
    
      plt.legend()
      plt.xlabel(b+"_test")
      
      plt.title("test")
      plt.savefig(f'./test/test_dae_{b}.jpg', bbox_inches="tight", pad_inches=0.0)
     # plt.show() 
      plt.clf()

#np.array(rec_train).shape
#np.array(Xtrn).shape
"""
if not os.path.exists('./tsne2d'):
  os.mkdir('./tsne2d') 

tsne = TSNE(n_components=2)
ori_tsne = tsne.fit_transform(Xtrn)
rec_tsne = tsne.fit_transform(rec_train)
rec_tsne_2 = tsne.fit_transform(rec_train_2)
rec_tsne_3= tsne.fit_transform(rec_train_3)
rec_tsne_4 = tsne.fit_transform(rec_train_4)
rec_tsne_5 = tsne.fit_transform(rec_train_5)


ori = pd.DataFrame(ori_tsne , columns=["Dim1", "Dim2"])
rec = pd.DataFrame(rec_tsne , columns=["Dim1", "Dim2"])
rec_2 = pd.DataFrame(rec_tsne_2 , columns=["Dim1", "Dim2"])
rec_3 = pd.DataFrame(rec_tsne_3 , columns=["Dim1", "Dim2"])
rec_4 = pd.DataFrame(rec_tsne_4 , columns=["Dim1", "Dim2"])
rec_5 = pd.DataFrame(rec_tsne_5 , columns=["Dim1", "Dim2"])


sns.scatterplot(data=ori, x="Dim1", y="Dim2", label="original")
sns.scatterplot(data=rec_5, x="Dim1", y="Dim2" ,label="reconstructed_MVAE")
plt.legend()
plt.savefig(f'./tsne2d/mvae2d.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()

sns.scatterplot(data=ori, x="Dim1", y="Dim2", label="original")
sns.scatterplot(data=rec, x="Dim1", y="Dim2" ,label="reconstructed_MemAE")
plt.legend()
plt.savefig(f'./tsne2d/Memae2d.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()
#plt.show()

sns.scatterplot(data=ori, x="Dim1", y="Dim2", label="original")
sns.scatterplot(data=rec_2, x="Dim1", y="Dim2" ,label="reconstructed_AE")
plt.legend()
plt.savefig(f'./tsne2d/ae2d.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()
#plt.show()

sns.scatterplot(data=ori, x="Dim1", y="Dim2", label="original")
sns.scatterplot(data=rec_3, x="Dim1", y="Dim2" ,label="reconstructed_DAE")
plt.legend()
plt.savefig(f'./tsne2d/dae2d.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()
#plt.show()

sns.scatterplot(data=ori, x="Dim1", y="Dim2", label="original")
sns.scatterplot(data=rec_4, x="Dim1", y="Dim2" ,label="reconstructed_VAE")
plt.legend()
plt.savefig(f'./tsne2d/vae2d.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()
#plt.show()
"""
"""
if not os.path.exists('./tsne3d'):
  os.mkdir('./tsne3d') 

tsne = TSNE(n_components=3)
ori_tsne = tsne.fit_transform(Xtrn)
rec_tsne = tsne.fit_transform(rec_train)
rec_tsne_2 = tsne.fit_transform(rec_train_2)
rec_tsne_3= tsne.fit_transform(rec_train_3)
rec_tsne_4 = tsne.fit_transform(rec_train_4)
rec_tsne_5 = tsne.fit_transform(rec_train_5)


ori_3d = pd.DataFrame(ori_tsne , columns=["Dim1", "Dim2","Dim3"])
rec_3d = pd.DataFrame(rec_tsne , columns=["Dim1", "Dim2","Dim3"])
rec_2_3d = pd.DataFrame(rec_tsne_2 , columns=["Dim1", "Dim2","Dim3"])
rec_3_3d = pd.DataFrame(rec_tsne_3 , columns=["Dim1", "Dim2","Dim3"])
rec_4_3d = pd.DataFrame(rec_tsne_4 , columns=["Dim1", "Dim2","Dim3"])
rec_5_3d = pd.DataFrame(rec_tsne_5 , columns=["Dim1", "Dim2","Dim3"])
    

axes = plt.axes(projection='3d')

axes.scatter3D(ori_3d["Dim1"], ori_3d["Dim2"], ori_3d["Dim3"], label='original')
axes.scatter3D(rec_5_3d["Dim1"], rec_5_3d["Dim2"], rec_5_3d["Dim3"], label='reconstructed_MVAE"')

axes.set_xlabel('Dim1')
axes.set_ylabel('Dim2')
axes.set_zlabel('Dim3')

plt.legend()
plt.savefig(f'./tsne3d/Mvae3d.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()

axes = plt.axes(projection='3d')

axes.scatter3D(ori_3d["Dim1"], ori_3d["Dim2"], ori_3d["Dim3"], label='original')
axes.scatter3D(rec_3d["Dim1"], rec_3d["Dim2"], rec_3d["Dim3"], label='reconstructed_MemAE"')

axes.set_xlabel('Dim1')
axes.set_ylabel('Dim2')
axes.set_zlabel('Dim3')

plt.legend()
plt.savefig(f'./tsne3d/Memae3d.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()
#plt.show()

axes = plt.axes(projection='3d')
axes.scatter3D(ori_3d["Dim1"], ori_3d["Dim2"], ori_3d["Dim3"], label='original')
axes.scatter3D(rec_2_3d["Dim1"], rec_2_3d["Dim2"], rec_2_3d["Dim3"], label='reconstructed_AE"')

axes.set_xlabel('Dim1')
axes.set_ylabel('Dim2')
axes.set_zlabel('Dim3')

plt.legend()
plt.savefig(f'./tsne3d/ae3d.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()
#plt.show()

axes = plt.axes(projection='3d')
axes.scatter3D(ori_3d["Dim1"], ori_3d["Dim2"], ori_3d["Dim3"], label='original')
axes.scatter3D(rec_3_3d["Dim1"], rec_3_3d["Dim2"], rec_3_3d["Dim3"], label='reconstructed_DAE"')

axes.set_xlabel('Dim1')
axes.set_ylabel('Dim2')
axes.set_zlabel('Dim3')

plt.legend()
plt.savefig(f'./tsne3d/dae3d.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()
#plt.show()

axes = plt.axes(projection='3d')
axes.scatter3D(ori_3d["Dim1"], ori_3d["Dim2"], ori_3d["Dim3"], label='original')
axes.scatter3D(rec_4_3d["Dim1"], rec_4_3d["Dim2"], rec_4_3d["Dim3"], label='reconstructed_VAE"')

axes.set_xlabel('Dim1')
axes.set_ylabel('Dim2')
axes.set_zlabel('Dim3')

plt.legend()
plt.savefig(f'./tsne3d/vae3d.jpg', bbox_inches="tight", pad_inches=0.0)
plt.clf()
#plt.show()

"""